{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_chatbot_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQ-NHhqYQiD",
        "colab_type": "text"
      },
      "source": [
        "# Asking ChatBot about chosen information\n",
        "\n",
        "\n",
        "*   What is..\n",
        "*   Tell me about..\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGpl2sGvbSFO",
        "colab_type": "text"
      },
      "source": [
        "## Libraries or concepts used in the process\n",
        " \n",
        "*   NLTK - a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "*   TF-IDF - statistical method of evaluating the significance of a word in a given document.\n",
        "\n",
        "*   Cosine similarity - denotes the similarity between the two words\n",
        "\n",
        "*   WordNet -  a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members.\n",
        "\n",
        "*   WordNetLemmatizer -  Lemmatize using WordNet's built-in morphy function. Lemmatization \n",
        "\n",
        "*   Wikipedia - Python library that makes it easy to access and parse data from Wikipedia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dtRZxYZlz0",
        "colab_type": "text"
      },
      "source": [
        "## Installs libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_LkfvPZBgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "c79b86ee-2dcc-4589-819a-16bf11c5de29"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8vYVLMGMszd",
        "colab_type": "text"
      },
      "source": [
        "## Imports libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoLUbJY4fTgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "34688ab0-86c9-4f5b-896e-40a5e507d951"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
        "import urllib.request\n",
        "import re\n",
        "from IPython.display import Image"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnFchprfPFL",
        "colab_type": "text"
      },
      "source": [
        "## Gets text data from url and cleans it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2--IZ-k7AcrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uf = urllib.request.urlopen(\"https://plato.stanford.edu/entries/meaning/\")\n",
        "html = uf.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAvefLrppdw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c90605d-cfbc-4b1d-e200-5f2956d1b0f8"
      },
      "source": [
        "html[:100]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ABljy9-YDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags and unnecessary characters\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('\\n|<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7ykxP9_PKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c76197a-bcbc-4877-875e-f01cdb6b3e34"
      },
      "source": [
        "# change to string\n",
        "html = html.decode(\"utf-8\")\n",
        "html[:100]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5KNvx8rU7S",
        "colab_type": "text"
      },
      "source": [
        "## Shows the beginning of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOBLhu99qmKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "89f5c1cc-ad9b-46d4-bc35-3c7a86e2f26f"
      },
      "source": [
        "html = cleanhtml(html)\n",
        "print(html[:100])\n",
        "raw = html.lower()\n",
        "print(raw[:100])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  -->  Theories of Meaning (Stanford Encyclopedia of Philosophy)                    \n",
            "                  -->  theories of meaning (stanford encyclopedia of philosophy)                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZDuOpVaF4J",
        "colab_type": "text"
      },
      "source": [
        "## Sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q55VS2-2Nfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyA9w_TE_6F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "deadb9a7-490b-4e8d-a699-9c01b72aa84f"
      },
      "source": [
        "sent_tokens[1:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unfortunately, this term has also been used to mean a greatnumber of different things.',\n",
              " 'in this entry, the focus is on two sortsof theory of meaning.',\n",
              " 'the first sort of theoryasemantic theoryis a theory which assigns semantic contents toexpressions of a language.',\n",
              " 'the second sort of theoryafoundational theory of meaningis a theory which states thefacts in virtue of which expressions have the semantic contents thatthey have.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpLM0OFFaHBM",
        "colab_type": "text"
      },
      "source": [
        "## Text normalisation\n",
        "\n",
        "Word tokenization\n",
        "\n",
        "*   Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
        "\n",
        "*   Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPv_-9-L_CXe",
        "colab_type": "text"
      },
      "source": [
        "## Working with Unicode\n",
        "Strings are usually easy to deal with when they are made up of English ASCII characters, but â€œproblemsâ€ appear when we enter into non-ASCII characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBtDihvx_6LK",
        "colab_type": "text"
      },
      "source": [
        "### What are strings made of?\n",
        "Byte is a unit of information that is built of 8 bits â€” bytes are used to store all files in a hard disk. So all of the CSVs and JSON files on your computer are built of bytes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAOQ37VJ2kX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1660107c-1da8-4734-f6cc-f7b5592971fd"
      },
      "source": [
        "# The ord() function returns an integer representing the Unicode character\n",
        "ord('ðŸ')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN0XYLJn2r68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2be4c8b-055c-464b-f396-054065df4504"
      },
      "source": [
        "# The chr() returns a character (a string) whose Unicode code point is the integer\n",
        "chr(128013)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ðŸ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HadI0Xdx_6su",
        "colab_type": "text"
      },
      "source": [
        "### ASCII\n",
        "\n",
        "\n",
        "\n",
        "* character encoding standard\n",
        "* 127 symbol list \n",
        "* cool for the initial few decades or so\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQv3WHreACcK",
        "colab_type": "text"
      },
      "source": [
        "### Unicode\n",
        "\n",
        "* International standard where a mapping of individual characters and a unique number is maintained\n",
        "* Over 137k characters including different scripts including English, Hindi, Chinese and Japanese, as well as emojis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ic_vpFXACwV",
        "colab_type": "text"
      },
      "source": [
        "### Unicode encodings UTF-8, UTF-16, and UTF-32\n",
        "\n",
        "\n",
        "*   UTF-8: It uses 1, 2, 3 or 4 bytes to encode every code point\n",
        "*   UTF-16 is variable 2 or 4 bytes, great for Asian text\n",
        "* UTF-32 is fixed 4 bytes, needs a lot of memory, not used very often\n",
        "\n",
        "decode() -> str <br>\n",
        "encode() -> bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfXDd9FcADGY",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn.bulldogjob.com/system/photos/files/000/005/268/original/1_nyvQSXsxG7cZILqZ8H5-Wg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM57N5MO_7F3",
        "colab_type": "text"
      },
      "source": [
        "### Example of encoding and decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEookpoiTvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "82cc4a72-9f4a-47ca-8509-1c98b7e77dcc"
      },
      "source": [
        "word = \"pythÃ¶n\"\n",
        "# unicodedata normalize return the normal form\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore'))\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'python'\n",
            "python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Y7MNwOuSqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cd54d9bc-4fa9-4857-daa0-ac578a717fe9"
      },
      "source": [
        "word = \"pythÃ¶n\"\n",
        "# ignore means that we do not replace odd character with anything\n",
        "print(word.encode('ascii', 'ignore'))\n",
        "print(word.encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'pythn'\n",
            "pythn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwmXC1-WMVvS",
        "colab_type": "text"
      },
      "source": [
        "## Text normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UOuyacCd6Rr",
        "colab_type": "text"
      },
      "source": [
        "## POS tagging\n",
        "\n",
        "*  one of the fundamental tasks of natural language processing tasks (eg. Word Sense Disambiguation)\n",
        "*  words often occur in different senses as different parts of speech, eg:\n",
        "\n",
        "1.  She saw a bear\n",
        "2.  Your efforts will bear fruit\n",
        "\n",
        "* completely different senses -> one is a noun and other is a verb. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_T5kGM5aCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    #word tokenization\n",
        "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "    #remove ascii\n",
        "    new_words = []\n",
        "    for word in word_token:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "\n",
        "    #remove tags\n",
        "    rmv = []\n",
        "    for w in new_words:\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "        rmv.append(text)\n",
        "        \n",
        "    #pos tagging and lemmatization (from nltk.corpus import wordnet as wn)\n",
        "    tag_map = defaultdict(lambda : wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    rmv = [i for i in rmv if i]\n",
        "    for token, tag in nltk.pos_tag(rmv):\n",
        "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "        lemma_list.append(lemma)\n",
        "    return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLOr9rZaIBQ",
        "colab_type": "text"
      },
      "source": [
        "## Creating greeting responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ShXA9BM5aPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining welcome input that will be recognized by bot\n",
        "welcome_input = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\"]\n",
        "# defining welcome output from bot \n",
        "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "            return random.choice(welcome_response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z23iUKMPaIwY",
        "colab_type": "text"
      },
      "source": [
        "## Generating response for the knowledge question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580njbbzRzsh",
        "colab_type": "text"
      },
      "source": [
        "### Term Frequency (TF)\n",
        "\n",
        "\n",
        "* The number of times a word appears in a document divded by the total number of words in the document\n",
        "\n",
        "### Inverse Data Frequency (IDF)\n",
        "\n",
        "\n",
        "* The log of the number of documents divided by the number of documents that contain the word w.\n",
        "* Inverse data frequency determines the weight of rare words across all documents in the corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZnJeiy5aZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    #vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "        #wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WsSQ-QTaJny",
        "colab_type": "text"
      },
      "source": [
        "## Running the bot while True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux_NlXFw5amZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "bf8a1f8f-860a-4f5f-e380-a09bf830e6c8"
      },
      "source": [
        "flag=True\n",
        "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    # changing input to lowercase\n",
        "    user_response=user_response.lower()\n",
        "    # checking if the user want to exit\n",
        "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Chatterbot : You are welcome..\")\n",
        "        else:\n",
        "            if user_response in welcome_input:\n",
        "                print(\"Chatterbot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chatterbot : \",end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Chatterbot : Bye!!! \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n",
            "Tell me about nothing\n",
            "Chatterbot : Checking Wikipedia\n",
            "\"Nothing\", used as a pronoun subject, is the absence of a something or particular thing that one might expect or desire to be present (\"We found nothing\", \"Nothing was there\") or the inactivity of a thing or things that are usually or could be active (\"Nothing moved\", \"Nothing happened\").  As a predicate or complement \"nothing\" is the absence of meaning, value, worth, relevance, standing, or significance (\"It is a tale/ Told by an idiot, full of sound and fury,/ Signifying nothing\"; \"The affair meant nothing\"; \"I'm nothing in their eyes\").  \"Nothingness\" is a philosophical term for the general state of nonexistence, sometimes reified as a domain or dimension into which things pass when they cease to exist or out of which they may come to exist, e.g., God is understood to have created the universe ex nihilo, \"out of nothing\".\n",
            "What is theory of meaning\n",
            "Chatterbot : just so, semantic theories and foundational theories ofmeaning are, pretty clearly, different sorts of theories.the term theory of meaning has, in the recent history ofphilosophy, been used to stand for both semantic theories andfoundational theories of meaning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UZi8rZm04Y",
        "colab_type": "text"
      },
      "source": [
        "### Ask what is:\n",
        "\n",
        "*   What is theory of meaning\n",
        "*   What is possible worlds semantics\n",
        "\n",
        "### Ask to tell:\n",
        "\n",
        "*   Tell me about nothing\n",
        "*   Tell me about human\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vJmfj1o5a9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Sources: https://towardsdatascience.com/lets-build-an-intelligent-chatbot-7ea7f215ada6,\n",
        "https://towardsdatascience.com/a-guide-to-unicode-utf-8-and-strings-in-python-757a232db95c'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}